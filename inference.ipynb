{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6fecd0e",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e195b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "%pip install -r requirements.txt\n",
    "\n",
    "# Make a directory for pretrained models\n",
    "!mkdir -p pretrained\n",
    "%cd pretrained\n",
    "\n",
    "# Download the model from Hugging Face into the directory\n",
    "!huggingface-cli download --resume-download \\\n",
    "  --local-dir-use-symlinks False \\\n",
    "  --local-dir InternVL3-78B \\\n",
    "  OpenGVLab/InternVL3-78B\n",
    "\n",
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "toy_ds = '/content/drive/My Drive/datasets/toy_ds'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241c4d49",
   "metadata": {},
   "source": [
    "## Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "283f33fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-dai7591/.conda/envs/videoqa/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/jupyter-dai7591/.conda/envs/videoqa/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Frame1: <image>\n",
      "Frame2: <image>\n",
      "Frame3: <image>\n",
      "Frame4: <image>\n",
      "Frame5: <image>\n",
      "Frame6: <image>\n",
      "Frame7: <image>\n",
      "Frame8: <image>\n",
      "Frame9: <image>\n",
      "Frame10: <image>\n",
      "Frame11: <image>\n",
      "Frame12: <image>\n",
      "Frame13: <image>\n",
      "Frame14: <image>\n",
      "Frame15: <image>\n",
      "Frame16: <image>\n",
      "Frame17: <image>\n",
      "Frame18: <image>\n",
      "Frame19: <image>\n",
      "Frame20: <image>\n",
      "Frame21: <image>\n",
      "Frame22: <image>\n",
      "Frame23: <image>\n",
      "Frame24: <image>\n",
      "Frame25: <image>\n",
      "Frame26: <image>\n",
      "Frame27: <image>\n",
      "Frame28: <image>\n",
      "Frame29: <image>\n",
      "Frame30: <image>\n",
      "Frame31: <image>\n",
      "Frame32: <image>\n",
      "Provide a detailed description of each image. Describe the foreground and background separately. Mention any people, objects, and actions clearly. What are the people doing? What expressions or activities are visible? What is the setting or context? Is there violence happening?\n",
      "Assistant: The video is a short, dynamic sequence of events unfolding outside a glass door at an indoor venue, possibly a commercial or governmental building. At the forefront is the tiled floor, likely within a building as indicated by the glass door. The sequence shows a group of people, some appearing to be police or security personnel, equipped with military-grade equipment such as rifles and helmets. They are gathered and standing at the entrance and upper level, seemingly observing something or moving cautiously under cover of darkness or at night.\n",
      "\n",
      "The individuals, dressed in camouflage and protective gear, are positioned at an elevated level and lower level, separated by a metallic guardrail. The scene includes a man in a beige jacket, a security guard or officer in black pants and a studded belt, and another person in a red shirt holding up a smartphone, suggesting surveillance or communication activities.\n",
      "\n",
      "The background outside the door shows parked cars, indicating a populated area or commercial space. The lighting is artificial, likely from interior lighting or nearby fixtures, casting reflections on the tiles. The video captures a tense or dynamic atmosphere, with the security presence hinting at potential intervention or action. However, there is no visible evidence of violence within the video itself. The overall mood appears to be controlled yet cautious, with everyone in position to observe whatever may be unfolding.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import torchvision.transforms as T\n",
    "from decord import VideoReader, cpu\n",
    "from PIL import Image\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
    "\n",
    "# ─── Prompt ─────────────────────────────────────────────────\n",
    "prompt_string = \"Provide a detailed description of each image. Describe the foreground and background separately. Mention any people, objects, and actions clearly. What are the people doing? What expressions or activities are visible? What is the setting or context? Is there violence happening?\"\n",
    "\n",
    "# ─── Model Setup 78B 8 bit quant ─────────────────────────────────────────────────\n",
    "MODEL_PATH = \"./pretrained/InternVL3-1B\"\n",
    "model = AutoModel.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    load_in_8bit=True,\n",
    "    low_cpu_mem_usage=True,\n",
    "    use_flash_attn=True,\n",
    "    trust_remote_code=True).eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)\n",
    "\n",
    "# ─── Constants ───────────────────────────────────────────────────\n",
    "INPUT_SIZE = 448\n",
    "NUM_FRAMES = 32\n",
    "GEN_CONFIG = dict(max_new_tokens=1024, do_sample=True)\n",
    "MEAN = (0.485, 0.456, 0.406)\n",
    "STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "# ─── Split Model Across GPUs ──────────────────────────────────────────────\n",
    "def split_model(model_name):\n",
    "    device_map = {}\n",
    "    world_size = torch.cuda.device_count()\n",
    "    config = AutoConfig.from_pretrained(model_name, trust_remote_code=True)\n",
    "    num_layers = config.llm_config.num_hidden_layers\n",
    "\n",
    "    # Distribute layers evenly, but give half of GPU0 to the vision part\n",
    "    num_layers_per_gpu = math.ceil(num_layers / (world_size - 0.5))\n",
    "    num_layers_per_gpu = [num_layers_per_gpu] * world_size\n",
    "    num_layers_per_gpu[0] = math.ceil(num_layers_per_gpu[0] * 0.5)\n",
    "\n",
    "    layer_cnt = 0\n",
    "    for gpu_idx, count in enumerate(num_layers_per_gpu):\n",
    "        for _ in range(count):\n",
    "            device_map[f'language_model.model.layers.{layer_cnt}'] = gpu_idx\n",
    "            layer_cnt += 1\n",
    "\n",
    "    # Pin all vision & shared embeddings to GPU0\n",
    "    vision_keys = [\n",
    "        'vision_model',\n",
    "        'mlp1',\n",
    "        'language_model.model.tok_embeddings',\n",
    "        'language_model.model.embed_tokens',\n",
    "        'language_model.output',\n",
    "        'language_model.model.norm',\n",
    "        'language_model.model.rotary_emb',\n",
    "        'language_model.lm_head',\n",
    "        f'language_model.model.layers.{num_layers - 1}'\n",
    "    ]\n",
    "    for key in vision_keys:\n",
    "        device_map[key] = 0\n",
    "\n",
    "    return device_map\n",
    "\n",
    "# ─── Transforms ─────────────────────────────────────────────────\n",
    "def build_transform():\n",
    "    return T.Compose([\n",
    "        T.Lambda(lambda img: img.convert('RGB')),\n",
    "        T.Resize((INPUT_SIZE, INPUT_SIZE), interpolation=InterpolationMode.BICUBIC),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=MEAN, std=STD)\n",
    "    ])\n",
    "\n",
    "# ─── Load and Preprocess Video ──────────────────────────────────\n",
    "def get_frame_indices(num_frames, total):\n",
    "    return np.linspace(0, total - 1, num_frames, dtype=int)\n",
    "\n",
    "def load_video(video_path, num_frames):\n",
    "    vr = VideoReader(video_path, ctx=cpu(0))\n",
    "    transform = build_transform()\n",
    "    indices = get_frame_indices(num_frames, len(vr))\n",
    "    pixel_values = [transform(Image.fromarray(vr[i].asnumpy())) for i in indices]\n",
    "    return torch.stack(pixel_values)  # [num_frames, 3, H, W]\n",
    "\n",
    "# ─── Inference ──────────────────────────────────────────────────\n",
    "def infer(video_path):\n",
    "    video_tensor = load_video(video_path, NUM_FRAMES).to(torch.bfloat16).cuda()\n",
    "    prompt = ''.join([f'Frame{i+1}: <image>\\n' for i in range(NUM_FRAMES)])\n",
    "    prompt += prompt_string\n",
    "\n",
    "    response, _ = model.chat(\n",
    "        tokenizer,\n",
    "        video_tensor,\n",
    "        prompt,\n",
    "        GEN_CONFIG,\n",
    "        history=None,\n",
    "        return_history=True\n",
    "    )\n",
    "    print(\"User:\", prompt)\n",
    "    print(\"Assistant:\", response)\n",
    "\n",
    "# ─── Entry ──────────────────────────────────────────────────────\n",
    "if __name__ == \"__main__\":\n",
    "    infer(\"./TNS_0119_V.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1aae66",
   "metadata": {},
   "source": [
    "## Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "620df763",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-dai7591/.conda/envs/videoqa/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: <image>\n",
      "Provide a detailed description of each image. Describe the foreground and background separately. Mention any people, objects, and actions clearly. What are the people doing? What expressions or activities are visible? What is the setting or context? Is there violence happening?\n",
      "Assistant: **Foreground:**\n",
      "\n",
      "1. **Person Standing:**\n",
      "   - **Gender:** Male\n",
      "   - **Appearance:** Short dark hair and wearing a light-colored, button-up shirt that is partially unbuttoned and draped loosely over his shoulders. He has dark pants with some damage and is barefoot.\n",
      "   - **Limbs:** Both hands are visible but not directly touching; he is holding a book in his left hand.\n",
      "   - **Attire:** The shirt is plain with some visible stains and patches, paired with darker jeans.\n",
      "   - **Footwear:** Dark flip-flops.\n",
      "\n",
      "2. **Actions:**\n",
      "   - The individual is holding what seems to be a book or paper.\n",
      "\n",
      "**Background:**\n",
      "\n",
      "1. **People:**\n",
      "   - There are several individuals visible in the background, appearing to be part of a military or security force.\n",
      "   - **First Person:** \n",
      "     - **Gender:** Appears to be male\n",
      "     - **Appearance:** Wearing a red beret, gray long-sleeved shirt, dark pants, and a light-colored cap.\n",
      "     - **Limbs:** Both hands and arms visible.\n",
      "     - **Footwear:** Dark pants. \n",
      "   - **Second Person:**\n",
      "     - **Gender:** Male\n",
      "     - **Appearance:** Wearing a beige military uniform, with a helmet, and seated.\n",
      "   - **Third Person:**\n",
      "     - **Gender:** Male\n",
      "     - **Appearance:** Seated and wearing a red hat and a watch on his left wrist.\n",
      "   - **Additional Individuals:**\n",
      "     - **Person in front of a vehicle:**\n",
      "       - **Gender:** Male\n",
      "       - **Appearance:** Wearing a mask and holding a blue container, standing next to a military vehicle.\n",
      "     - **Individual in front left side:**\n",
      "       - **Gender:** Female\n",
      "       - **Appearance:** Wearing a colorful headscarf with red and green vertical stripes, denim trousers, and white sandals or shoes.\n",
      "   \n",
      "2. **Actions:**\n",
      "   - The individuals appear to be gathered around the military structure or vehicle.\n",
      "   - **First Person:** Sitting on the bench next to the vehicle, with a rifle held up.\n",
      "   - **Other Individuals:** Some of them are conversing or engaging as they watch the first person.\n",
      "\n",
      "3. **Expressions or Activities:**\n",
      "   - The overall mood of the scene is non-aggressive; the individuals seem calmly interacting.\n",
      "\n",
      "4. **Setting:**\n",
      "   - The scene appears to be outdoors, on a paved street with a curb.\n",
      "   - The background shows a building with windows, barred windows, and a gate.\n",
      "\n",
      "5. **Objects:**\n",
      "   - A military vehicle with multiple military personnel, visible armaments, and flags on the roof.\n",
      "   - Various items on the ground, including plastic bags and a flag with green, white, red, and black stripes.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
    "\n",
    "# ─── Prompt ─────────────────────────────────────────────────\n",
    "prompt_string = \"Provide a detailed description of each image. Describe the foreground and background separately. Mention any people, objects, and actions clearly. What are the people doing? What expressions or activities are visible? What is the setting or context? Is there violence happening?\"\n",
    "\n",
    "# ─── Config ───────────────────────────────────────────────────────────────\n",
    "MODEL_PATH = \"./pretrained/InternVL3-1B\"\n",
    "INPUT_SIZE   = 448\n",
    "MAX_PATCHES  = 12\n",
    "GEN_CONFIG   = dict(max_new_tokens=1024, do_sample=True)\n",
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD  = (0.229, 0.224, 0.225)\n",
    "\n",
    "# ─── Split Model Across GPUs ──────────────────────────────────────────────\n",
    "def split_model(model_name):\n",
    "    device_map = {}\n",
    "    world_size = torch.cuda.device_count()\n",
    "    config = AutoConfig.from_pretrained(model_name, trust_remote_code=True)\n",
    "    num_layers = config.llm_config.num_hidden_layers\n",
    "\n",
    "    # Distribute layers evenly, but give half of GPU0 to the vision part\n",
    "    num_layers_per_gpu = math.ceil(num_layers / (world_size - 0.5))\n",
    "    num_layers_per_gpu = [num_layers_per_gpu] * world_size\n",
    "    num_layers_per_gpu[0] = math.ceil(num_layers_per_gpu[0] * 0.5)\n",
    "\n",
    "    layer_cnt = 0\n",
    "    for gpu_idx, count in enumerate(num_layers_per_gpu):\n",
    "        for _ in range(count):\n",
    "            device_map[f'language_model.model.layers.{layer_cnt}'] = gpu_idx\n",
    "            layer_cnt += 1\n",
    "\n",
    "    # Pin all vision & shared embeddings to GPU0\n",
    "    vision_keys = [\n",
    "        'vision_model',\n",
    "        'mlp1',\n",
    "        'language_model.model.tok_embeddings',\n",
    "        'language_model.model.embed_tokens',\n",
    "        'language_model.output',\n",
    "        'language_model.model.norm',\n",
    "        'language_model.model.rotary_emb',\n",
    "        'language_model.lm_head',\n",
    "        f'language_model.model.layers.{num_layers - 1}'\n",
    "    ]\n",
    "    for key in vision_keys:\n",
    "        device_map[key] = 0\n",
    "\n",
    "    return device_map\n",
    "\n",
    "# ─── Image Preprocessing ─────────────────────────────────────────────────\n",
    "def build_transform():\n",
    "    return T.Compose([\n",
    "        T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
    "        T.Resize((INPUT_SIZE, INPUT_SIZE), interpolation=InterpolationMode.BICUBIC),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "    ])\n",
    "\n",
    "def dynamic_preprocess(image, image_size=448, max_num=12):\n",
    "    orig_w, orig_h = image.size\n",
    "    aspect = orig_w / orig_h\n",
    "\n",
    "    # find best grid (i × j) closest to aspect, with i*j ≤ max_num\n",
    "    best, best_diff = (1,1), float('inf')\n",
    "    for i in range(1, max_num+1):\n",
    "        for j in range(1, max_num+1):\n",
    "            if i*j > max_num: continue\n",
    "            diff = abs(aspect - (i/j))\n",
    "            if diff < best_diff:\n",
    "                best, best_diff = (i,j), diff\n",
    "\n",
    "    gw, gh = best\n",
    "    new_w, new_h = image_size * gw, image_size * gh\n",
    "    image = image.resize((new_w, new_h))\n",
    "\n",
    "    tiles = []\n",
    "    for y in range(gh):\n",
    "        for x in range(gw):\n",
    "            box = (x*image_size, y*image_size, (x+1)*image_size, (y+1)*image_size)\n",
    "            tiles.append(image.crop(box))\n",
    "    return tiles\n",
    "\n",
    "def load_image(path, image_size=448, max_num=12):\n",
    "    img = Image.open(path).convert('RGB')\n",
    "    tiles = dynamic_preprocess(img, image_size=image_size, max_num=max_num)\n",
    "    tfm = build_transform()\n",
    "    return torch.stack([tfm(t) for t in tiles])\n",
    "\n",
    "# ─── Inference ────────────────────────────────────────────────────────────\n",
    "def infer(image_path):\n",
    "    device_map = split_model(MODEL_PATH)\n",
    "    model     = AutoModel.from_pretrained(\n",
    "                    MODEL_PATH,\n",
    "                    torch_dtype=torch.bfloat16,\n",
    "                    low_cpu_mem_usage=True,\n",
    "                    use_flash_attn=True,\n",
    "                    trust_remote_code=True,\n",
    "                    device_map=device_map\n",
    "                ).eval()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True, use_fast=False)\n",
    "\n",
    "    pixel_values = load_image(image_path, max_num=MAX_PATCHES)\n",
    "    pixel_values = pixel_values.to(torch.bfloat16).cuda()\n",
    "\n",
    "    question = f\"<image>\\n{prompt_string}\"\n",
    "    response, _ = model.chat(tokenizer, pixel_values, question, GEN_CONFIG, history=None, return_history=True)\n",
    "\n",
    "    print(\"User:\", question)\n",
    "    print(\"Assistant:\", response)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    infer(\"toy_ds/images/TNS_3773_I.jpg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "videoqa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
