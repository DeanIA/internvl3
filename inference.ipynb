{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c6fecd0e",
      "metadata": {
        "id": "c6fecd0e"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "7e195b9e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7e195b9e",
        "outputId": "59fa2133-c594-4879-f04e-90a6afcfe35a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# # Make a directory for pretrained models\n",
        "# !mkdir -p pretrained\n",
        "# %cd pretrained\n",
        "\n",
        "# # Download the model from Hugging Face into the directory\n",
        "# !huggingface-cli download --resume-download \\\n",
        "#   --local-dir-use-symlinks False \\\n",
        "#   --local-dir InternVL3-78B \\\n",
        "#   OpenGVLab/InternVL3-78B\n",
        "\n",
        "\n",
        "# # Install the packages\n",
        "# %pip install -r '/content/drive/MyDrive/toy_ds/requirements.txt'\n",
        "\n",
        "# # 2️⃣ Install bitsandbytes (GPU-quant):\n",
        "# %pip install bitsandbytes==0.46.0\n",
        "\n",
        "# # 3️⃣ Install FlashAttention from its prebuilt wheel to skip the slow build\n",
        "# %pip install flash-attn==2.8.0.post2 --no-build-isolation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "toy_ds = '/content/drive/My Drive/datasets/InternVL/toy_ds'\n",
        "MODEL_PATH = '/content/My Drive/InterVL/InternVL3-78B'"
      ],
      "metadata": {
        "id": "Zvm8XSQGZOb2",
        "outputId": "5df36a7f-5982-4a6a-dc7e-e304c01a08fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Zvm8XSQGZOb2",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "241c4d49",
      "metadata": {
        "id": "241c4d49"
      },
      "source": [
        "## Video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "283f33fc",
      "metadata": {
        "id": "283f33fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 597
        },
        "outputId": "5bc56957-fda5-4d86-d582-1168e2461c4f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "Incorrect path_or_model_id: '/content/pretrained/InternVL3-78B'. Please provide either the path to a local folder or the repo_id of a model on the Hub.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    398\u001b[0m         \u001b[0;31m# Load from URL or cache if already cached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m         resolved_file = hf_hub_download(\n\u001b[0m\u001b[1;32m    400\u001b[0m             \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0marg_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"repo_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"from_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"to_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 \u001b[0mvalidate_repo_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrepo_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         raise HFValidationError(\n\u001b[0m\u001b[1;32m    155\u001b[0m             \u001b[0;34m\"Repo id must be in the form 'repo_name' or 'namespace/repo_name':\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/content/pretrained/InternVL3-78B'. Use `repo_type` argument if needed.",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-138287542>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# ─── Model Setup 78B 8 bit quant ─────────────────────────────────────────────────\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mMODEL_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/pretrained/InternVL3-78B\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m model = AutoModel.from_pretrained(\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mMODEL_PATH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbfloat16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m                 \u001b[0;31m# We make a call to the config file first (which may be absent) to get the commit hash as soon as possible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m                 resolved_config_file = cached_file(\n\u001b[0m\u001b[1;32m    485\u001b[0m                     \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m                     \u001b[0mCONFIG_NAME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    461\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"There was a specific connection error when trying to load {path_or_repo_id}:\\n{err}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHFValidationError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         raise EnvironmentError(\n\u001b[0m\u001b[1;32m    464\u001b[0m             \u001b[0;34mf\"Incorrect path_or_model_id: '{path_or_repo_id}'. Please provide either the path to a local folder or the repo_id of a model on the Hub.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m         ) from e\n",
            "\u001b[0;31mOSError\u001b[0m: Incorrect path_or_model_id: '/content/pretrained/InternVL3-78B'. Please provide either the path to a local folder or the repo_id of a model on the Hub."
          ]
        }
      ],
      "source": [
        "import os\n",
        "import math\n",
        "import torch\n",
        "import numpy as np\n",
        "import torchvision.transforms as T\n",
        "from decord import VideoReader, cpu\n",
        "from PIL import Image\n",
        "from torchvision.transforms.functional import InterpolationMode\n",
        "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
        "\n",
        "# ─── Prompt ─────────────────────────────────────────────────\n",
        "prompt_string = \"\"\"\n",
        "Please write one detailed but concise paragraph describing the following, based on the video frames or images above:\n",
        "- The overall scene and weather conditions (e.g., rain, puddles).\n",
        "- Any visible weapons or use of force:\n",
        "  • Guns (how many and where)\n",
        "  • Non-lethal force (e.g., hitting with fists, stones, or sticks)\n",
        "- Vehicles in the scene:\n",
        "  • Presence of military vehicles (specify type)\n",
        "  • License plates or markings (quote exact text)\n",
        "- Human activity:\n",
        "  • Actions of uniformed personnel (include uniform colors)\n",
        "  • Actions of protestors\n",
        "  • Whether anyone appears to be injured, hurt, or lying on the ground\n",
        "\"\"\"\n",
        "# ─── Model Setup 78B 8 bit quant ─────────────────────────────────────────────────\n",
        "\n",
        "model = AutoModel.from_pretrained(\n",
        "    MODEL_PATH,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    load_in_4bit=True,\n",
        "    low_cpu_mem_usage=True,\n",
        "    use_flash_attn=True,\n",
        "    trust_remote_code=True).eval()\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)\n",
        "\n",
        "# ─── Constants ───────────────────────────────────────────────────\n",
        "INPUT_SIZE = 448\n",
        "NUM_FRAMES = 32\n",
        "GEN_CONFIG = dict(max_new_tokens=1024, do_sample=True)\n",
        "MEAN = (0.485, 0.456, 0.406)\n",
        "STD = (0.229, 0.224, 0.225)\n",
        "\n",
        "# ─── Split Model Across GPUs ──────────────────────────────────────────────\n",
        "def split_model(model_name):\n",
        "    device_map = {}\n",
        "    world_size = torch.cuda.device_count()\n",
        "    config = AutoConfig.from_pretrained(model_name, trust_remote_code=True)\n",
        "    num_layers = config.llm_config.num_hidden_layers\n",
        "\n",
        "    # Distribute layers evenly, but give half of GPU0 to the vision part\n",
        "    num_layers_per_gpu = math.ceil(num_layers / (world_size - 0.5))\n",
        "    num_layers_per_gpu = [num_layers_per_gpu] * world_size\n",
        "    num_layers_per_gpu[0] = math.ceil(num_layers_per_gpu[0] * 0.5)\n",
        "\n",
        "    layer_cnt = 0\n",
        "    for gpu_idx, count in enumerate(num_layers_per_gpu):\n",
        "        for _ in range(count):\n",
        "            device_map[f'language_model.model.layers.{layer_cnt}'] = gpu_idx\n",
        "            layer_cnt += 1\n",
        "\n",
        "    # Pin all vision & shared embeddings to GPU0\n",
        "    vision_keys = [\n",
        "        'vision_model',\n",
        "        'mlp1',\n",
        "        'language_model.model.tok_embeddings',\n",
        "        'language_model.model.embed_tokens',\n",
        "        'language_model.output',\n",
        "        'language_model.model.norm',\n",
        "        'language_model.model.rotary_emb',\n",
        "        'language_model.lm_head',\n",
        "        f'language_model.model.layers.{num_layers - 1}'\n",
        "    ]\n",
        "    for key in vision_keys:\n",
        "        device_map[key] = 0\n",
        "\n",
        "    return device_map\n",
        "\n",
        "# ─── Transforms ─────────────────────────────────────────────────\n",
        "def build_transform():\n",
        "    return T.Compose([\n",
        "        T.Lambda(lambda img: img.convert('RGB')),\n",
        "        T.Resize((INPUT_SIZE, INPUT_SIZE), interpolation=InterpolationMode.BICUBIC),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize(mean=MEAN, std=STD)\n",
        "    ])\n",
        "\n",
        "# ─── Load and Preprocess Video ──────────────────────────────────\n",
        "def get_frame_indices(num_frames, total):\n",
        "    return np.linspace(0, total - 1, num_frames, dtype=int)\n",
        "\n",
        "def load_video(video_path, num_frames):\n",
        "    vr = VideoReader(video_path, ctx=cpu(0))\n",
        "    transform = build_transform()\n",
        "    indices = get_frame_indices(num_frames, len(vr))\n",
        "    pixel_values = [transform(Image.fromarray(vr[i].asnumpy())) for i in indices]\n",
        "    return torch.stack(pixel_values)  # [num_frames, 3, H, W]\n",
        "\n",
        "# ─── Inference ──────────────────────────────────────────────────\n",
        "def infer(video_path):\n",
        "    video_tensor = load_video(video_path, NUM_FRAMES).to(torch.bfloat16).cuda()\n",
        "    prompt = ''.join([f'Frame{i+1}: <image>\\n' for i in range(NUM_FRAMES)])\n",
        "    prompt += prompt_string\n",
        "\n",
        "    response, _ = model.chat(\n",
        "        tokenizer,\n",
        "        video_tensor,\n",
        "        prompt,\n",
        "        GEN_CONFIG,\n",
        "        history=None,\n",
        "        return_history=True\n",
        "    )\n",
        "    print(\"User:\", prompt)\n",
        "    print(\"Assistant:\", response)\n",
        "\n",
        "# ─── Entry ──────────────────────────────────────────────────────\n",
        "if __name__ == \"__main__\":\n",
        "    infer(\"/content/drive/MyDrive/toy_ds/videos/TNS_0169_V.mp4\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a1aae66",
      "metadata": {
        "id": "9a1aae66"
      },
      "source": [
        "## Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "620df763",
      "metadata": {
        "id": "620df763"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "import torchvision.transforms as T\n",
        "from PIL import Image\n",
        "from torchvision.transforms.functional import InterpolationMode\n",
        "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
        "\n",
        "# ─── Prompt ─────────────────────────────────────────────────\n",
        "prompt_string = \"Provide a detailed description of each image. Describe the foreground and background separately. Mention any people, objects, and actions clearly. What are the people doing? What expressions or activities are visible? What is the setting or context? Is there violence happening?\"\n",
        "\n",
        "# ─── Config ───────────────────────────────────────────────────────────────\n",
        "MODEL_PATH = \"./pretrained/InternVL3-78B\"\n",
        "INPUT_SIZE   = 448\n",
        "MAX_PATCHES  = 12\n",
        "GEN_CONFIG   = dict(max_new_tokens=1024, do_sample=True)\n",
        "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
        "IMAGENET_STD  = (0.229, 0.224, 0.225)\n",
        "\n",
        "# ─── Split Model Across GPUs ──────────────────────────────────────────────\n",
        "def split_model(model_name):\n",
        "    device_map = {}\n",
        "    world_size = torch.cuda.device_count()\n",
        "    config = AutoConfig.from_pretrained(model_name, trust_remote_code=True)\n",
        "    num_layers = config.llm_config.num_hidden_layers\n",
        "\n",
        "    # Distribute layers evenly, but give half of GPU0 to the vision part\n",
        "    num_layers_per_gpu = math.ceil(num_layers / (world_size - 0.5))\n",
        "    num_layers_per_gpu = [num_layers_per_gpu] * world_size\n",
        "    num_layers_per_gpu[0] = math.ceil(num_layers_per_gpu[0] * 0.5)\n",
        "\n",
        "    layer_cnt = 0\n",
        "    for gpu_idx, count in enumerate(num_layers_per_gpu):\n",
        "        for _ in range(count):\n",
        "            device_map[f'language_model.model.layers.{layer_cnt}'] = gpu_idx\n",
        "            layer_cnt += 1\n",
        "\n",
        "    # Pin all vision & shared embeddings to GPU0\n",
        "    vision_keys = [\n",
        "        'vision_model',\n",
        "        'mlp1',\n",
        "        'language_model.model.tok_embeddings',\n",
        "        'language_model.model.embed_tokens',\n",
        "        'language_model.output',\n",
        "        'language_model.model.norm',\n",
        "        'language_model.model.rotary_emb',\n",
        "        'language_model.lm_head',\n",
        "        f'language_model.model.layers.{num_layers - 1}'\n",
        "    ]\n",
        "    for key in vision_keys:\n",
        "        device_map[key] = 0\n",
        "\n",
        "    return device_map\n",
        "\n",
        "# ─── Image Preprocessing ─────────────────────────────────────────────────\n",
        "def build_transform():\n",
        "    return T.Compose([\n",
        "        T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
        "        T.Resize((INPUT_SIZE, INPUT_SIZE), interpolation=InterpolationMode.BICUBIC),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
        "    ])\n",
        "\n",
        "def dynamic_preprocess(image, image_size=448, max_num=12):\n",
        "    orig_w, orig_h = image.size\n",
        "    aspect = orig_w / orig_h\n",
        "\n",
        "    # find best grid (i × j) closest to aspect, with i*j ≤ max_num\n",
        "    best, best_diff = (1,1), float('inf')\n",
        "    for i in range(1, max_num+1):\n",
        "        for j in range(1, max_num+1):\n",
        "            if i*j > max_num: continue\n",
        "            diff = abs(aspect - (i/j))\n",
        "            if diff < best_diff:\n",
        "                best, best_diff = (i,j), diff\n",
        "\n",
        "    gw, gh = best\n",
        "    new_w, new_h = image_size * gw, image_size * gh\n",
        "    image = image.resize((new_w, new_h))\n",
        "\n",
        "    tiles = []\n",
        "    for y in range(gh):\n",
        "        for x in range(gw):\n",
        "            box = (x*image_size, y*image_size, (x+1)*image_size, (y+1)*image_size)\n",
        "            tiles.append(image.crop(box))\n",
        "    return tiles\n",
        "\n",
        "def load_image(path, image_size=448, max_num=12):\n",
        "    img = Image.open(path).convert('RGB')\n",
        "    tiles = dynamic_preprocess(img, image_size=image_size, max_num=max_num)\n",
        "    tfm = build_transform()\n",
        "    return torch.stack([tfm(t) for t in tiles])\n",
        "\n",
        "# ─── Inference ────────────────────────────────────────────────────────────\n",
        "def infer(image_path):\n",
        "    device_map = split_model(MODEL_PATH)\n",
        "    model     = AutoModel.from_pretrained(\n",
        "                    MODEL_PATH,\n",
        "                    torch_dtype=torch.bfloat16,\n",
        "                    low_cpu_mem_usage=True,\n",
        "                    use_flash_attn=True,\n",
        "                    trust_remote_code=True,\n",
        "                    device_map=device_map\n",
        "                ).eval()\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True, use_fast=False)\n",
        "\n",
        "    pixel_values = load_image(image_path, max_num=MAX_PATCHES)\n",
        "    pixel_values = pixel_values.to(torch.bfloat16).cuda()\n",
        "\n",
        "    question = f\"<image>\\n{prompt_string}\"\n",
        "    response, _ = model.chat(tokenizer, pixel_values, question, GEN_CONFIG, history=None, return_history=True)\n",
        "\n",
        "    print(\"User:\", question)\n",
        "    print(\"Assistant:\", response)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    infer(\"toy_ds/images/TNS_3773_I.jpg\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}