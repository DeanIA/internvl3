{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c6fecd0e",
      "metadata": {
        "id": "c6fecd0e"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "82cf2f21",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/jupyter-dai7591/.conda/envs/videoqa/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "Loading checkpoint shards: 100%|██████████| 7/7 [00:06<00:00,  1.01it/s]\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import torchvision.transforms as T\n",
        "from decord import VideoReader, cpu\n",
        "from PIL import Image\n",
        "from torchvision.transforms.functional import InterpolationMode\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "import json\n",
        "\n",
        "\n",
        "# ─── Model Setup 38B 8 bit quant ─────────────────────────────────────────────────\n",
        "MODEL_PATH = 'pretrained/InternVL3-14B'\n",
        "model = AutoModel.from_pretrained(\n",
        "    MODEL_PATH,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    load_in_8bit=True,\n",
        "    low_cpu_mem_usage=True,\n",
        "    use_flash_attn=True,\n",
        "    trust_remote_code=True).eval()\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "241c4d49",
      "metadata": {
        "id": "241c4d49"
      },
      "source": [
        "## Video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23ffe2e0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ─── Variables ───────────────────────────────────────────────────\n",
        "num_frames = 32\n",
        "temp = 0.1\n",
        "# ─── Prompt ─────────────────────────────────────────────────\n",
        "prompt_string = \"\"\"\n",
        "Be an objective Visual Evidence Analyst. Report on visual/audible data only. \n",
        "Use this template.\n",
        "\n",
        "Summary:\n",
        "- Overview and chronological events.\n",
        "\n",
        "- Military/Police:\n",
        "  - Appearance: Uniform colors, is there a visable insignia, gear.\n",
        "  - Weapons: Firearm or sticks.\n",
        "  - Force: All instances of physical force.\n",
        "  - Interaction: Commands, arrests, aid etc.\n",
        "- Civilians:\n",
        "  - Actions: Protesting, fleeing, throwing objects, etc.\n",
        "  - Condition: Visible injuries, distress, on ground.\n",
        "\n",
        "- Vehicles:\n",
        "  - Type: Civilian, police, military.\n",
        "  - Markings: Transcribe & translate.\n",
        "- Written Materials:\n",
        "  - Content: Banners, signs, graffiti.\n",
        "  - Translation: Quote & translate.\n",
        "- Conditions:\n",
        "  - Environment: Puddles on the ground indicating rain. \n",
        "\"\"\"\n",
        "\n",
        "# ─── Constants ───────────────────────────────────────────────────\n",
        "\n",
        "GEN_CONFIG = dict(max_new_tokens=1024, do_sample=True, temperature=temp)\n",
        "INPUT_SIZE = 448\n",
        "MEAN = (0.485, 0.456, 0.406)\n",
        "STD = (0.229, 0.224, 0.225)\n",
        "MAX_PATCHES  = 12\n",
        "OUTPUT_JSON = \"video_descriptions.json\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96fc60fb",
      "metadata": {},
      "source": [
        "## Helpers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "283f33fc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 597
        },
        "id": "283f33fc",
        "outputId": "5bc56957-fda5-4d86-d582-1168e2461c4f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (8628 > 8192). Running this sequence through the model will result in indexing errors\n",
            "/home/jupyter-dai7591/.conda/envs/videoqa/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
          ]
        }
      ],
      "source": [
        "# ─── Video  Process ──────────────────────────────────\n",
        "def transform_video():\n",
        "    return T.Compose([\n",
        "        T.Lambda(lambda img: img.convert('RGB')),\n",
        "        T.Resize((INPUT_SIZE, INPUT_SIZE), interpolation=InterpolationMode.BICUBIC),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize(mean=MEAN, std=STD)\n",
        "    ])\n",
        "\n",
        "def get_frame_indices(num_frames, total):\n",
        "    return np.linspace(0, total - 1, num_frames, dtype=int)\n",
        "\n",
        "def load_video(video_path, num_frames):\n",
        "    vr = VideoReader(video_path, ctx=cpu(0))\n",
        "    transform = transform_video()\n",
        "    indices = get_frame_indices(num_frames, len(vr))\n",
        "    video_tensor = [transform(Image.fromarray(vr[i].asnumpy())) for i in indices]\n",
        "    return torch.stack(video_tensor)  # [num_frames, 3, H, W]\n",
        "\n",
        "def infer_video(video_path):\n",
        "    try:\n",
        "        video_tensor = load_video(video_path, num_frames).to(torch.bfloat16).cuda()\n",
        "        video_tensor = video_tensor.contiguous()\n",
        "\n",
        "        prompt = ''.join([f'Frame{i+1}: <image>\\n' for i in range(num_frames)])\n",
        "        prompt += prompt_string\n",
        "\n",
        "        response, _ = model.chat(\n",
        "            tokenizer,\n",
        "            video_tensor,\n",
        "            prompt,\n",
        "            GEN_CONFIG,\n",
        "            history=None,\n",
        "            return_history=True\n",
        "        )\n",
        "        return response\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\"\n",
        "\n",
        "\n",
        "\n",
        "# ─── Image Preprocessing ─────────────────────────────────────────────────\n",
        "def transform_img():\n",
        "    return T.Compose([\n",
        "        T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
        "        T.Resize((INPUT_SIZE, INPUT_SIZE), interpolation=InterpolationMode.BICUBIC),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize(mean=MEAN, std=STD),\n",
        "    ])\n",
        "\n",
        "def dynamic_preprocess(image, image_size=448, max_num=12):\n",
        "    orig_w, orig_h = image.size\n",
        "    aspect = orig_w / orig_h\n",
        "\n",
        "    # find best grid (i × j) closest to aspect, with i*j ≤ max_num\n",
        "    best, best_diff = (1,1), float('inf')\n",
        "    for i in range(1, max_num+1):\n",
        "        for j in range(1, max_num+1):\n",
        "            if i*j > max_num: continue\n",
        "            diff = abs(aspect - (i/j))\n",
        "            if diff < best_diff:\n",
        "                best, best_diff = (i,j), diff\n",
        "\n",
        "    gw, gh = best\n",
        "    new_w, new_h = image_size * gw, image_size * gh\n",
        "    image = image.resize((new_w, new_h))\n",
        "\n",
        "    tiles = []\n",
        "    for y in range(gh):\n",
        "        for x in range(gw):\n",
        "            box = (x*image_size, y*image_size, (x+1)*image_size, (y+1)*image_size)\n",
        "            tiles.append(image.crop(box))\n",
        "    return tiles\n",
        "\n",
        "def load_image(path, image_size=448, max_num=12):\n",
        "    img = Image.open(path).convert('RGB')\n",
        "    tiles = dynamic_preprocess(img, image_size=image_size, max_num=max_num)\n",
        "    tfm = transform_img()\n",
        "    return torch.stack([tfm(t) for t in tiles])\n",
        "\n",
        "# ─── Inference ────────────────────────────────────────────────────────────\n",
        "def infer_img(image_path):\n",
        "    try:\n",
        "        pixel_values = load_image(image_path, max_num=MAX_PATCHES).to(torch.bfloat16).cuda()\n",
        "        pixel_values = pixel_values.contiguous()\n",
        "\n",
        "\n",
        "        question = f\"<image>\\n{prompt_string}\"\n",
        "        response, _ = model.chat(tokenizer, \n",
        "                                pixel_values, \n",
        "                                question, \n",
        "                                GEN_CONFIG, \n",
        "                                history=None, \n",
        "                                return_history=True)\n",
        "\n",
        "        return response\n",
        "    \n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\"\n",
        "\n",
        "# ─── Batch Loop ──────────────────────────────────────────────\n",
        "def batch_infer(video_dir):\n",
        "    results = {}\n",
        "    for filename in sorted(os.listdir(video_dir)):\n",
        "        if filename.lower().endswith(\".mp4\"):\n",
        "            path = os.path.join(video_dir, filename)\n",
        "            description = infer_video(path)\n",
        "            results[filename] = description\n",
        "\n",
        "        elif filename.lower().endswith('.jpg'):\n",
        "            path = os.path.join(video_dir, filename)\n",
        "            description = infer_img(path)\n",
        "            results[filename] = description\n",
        "    return results\n",
        "\n",
        "# ─── Main ───────────────────────────────────────────────────\n",
        "if __name__ == \"__main__\":\n",
        "    results = batch_infer('toy_ds')\n",
        "    with open(OUTPUT_JSON, \"w\") as f:\n",
        "        json.dump(results, f, indent=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4d87055",
      "metadata": {},
      "source": [
        "## Multiple GPUs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "620df763",
      "metadata": {
        "id": "620df763"
      },
      "outputs": [],
      "source": [
        "# import math\n",
        "# from transformers import  AutoConfig\n",
        "\n",
        "# # ─── Split Model Across GPUs ──────────────────────────────────────────────\n",
        "# def split_model(model_name):\n",
        "#     device_map = {}\n",
        "#     world_size = torch.cuda.device_count()\n",
        "#     config = AutoConfig.from_pretrained(model_name, trust_remote_code=True)\n",
        "#     num_layers = config.llm_config.num_hidden_layers\n",
        "\n",
        "#     # Distribute layers evenly, but give half of GPU0 to the vision part\n",
        "#     num_layers_per_gpu = math.ceil(num_layers / (world_size - 0.5))\n",
        "#     num_layers_per_gpu = [num_layers_per_gpu] * world_size\n",
        "#     num_layers_per_gpu[0] = math.ceil(num_layers_per_gpu[0] * 0.5)\n",
        "\n",
        "#     layer_cnt = 0\n",
        "#     for gpu_idx, count in enumerate(num_layers_per_gpu):\n",
        "#         for _ in range(count):\n",
        "#             device_map[f'language_model.model.layers.{layer_cnt}'] = gpu_idx\n",
        "#             layer_cnt += 1\n",
        "\n",
        "#     # Pin all vision & shared embeddings to GPU0\n",
        "#     vision_keys = [\n",
        "#         'vision_model',\n",
        "#         'mlp1',\n",
        "#         'language_model.model.tok_embeddings',\n",
        "#         'language_model.model.embed_tokens',\n",
        "#         'language_model.output',\n",
        "#         'language_model.model.norm',\n",
        "#         'language_model.model.rotary_emb',\n",
        "#         'language_model.lm_head',\n",
        "#         f'language_model.model.layers.{num_layers - 1}'\n",
        "#     ]\n",
        "#     for key in vision_keys:\n",
        "#         device_map[key] = 0\n",
        "\n",
        "#     return device_map\n",
        "\n",
        "# # ─── Inference ────────────────────────────────────────────────────────────\n",
        "# def infer(image_path):\n",
        "#     device_map = split_model(MODEL_PATH)\n",
        "#     model     = AutoModel.from_pretrained(\n",
        "#                     MODEL_PATH,\n",
        "#                     torch_dtype=torch.bfloat16,\n",
        "#                     low_cpu_mem_usage=True,\n",
        "#                     use_flash_attn=True,\n",
        "#                     trust_remote_code=True,\n",
        "#                     device_map=device_map\n",
        "#                 ).eval()\n",
        "#     tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True, use_fast=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f37734b7",
      "metadata": {},
      "source": [
        "## Context window"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e062a958",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "▶ Processing TNS_0001_V.mp4 (1167.7s)\n",
            " Segment   0.0s –  120.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/jupyter-dai7591/.conda/envs/videoqa/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Chunk-by-chunk video description with context-passing\n",
        "====================================================\n",
        "\n",
        "▪ Uses the existing helpers you pasted (`build_transform`, `dynamic_preprocess`,\n",
        "  `get_index`, etc.).\n",
        "▪ Passes **only the previous chunk’s summary** to the next chunk, so the prompt\n",
        "  length stays fixed.\n",
        "▪ Saves all chunk summaries to a single JSON file (`video_descriptions.json`).\n",
        "\n",
        "Drop this *below* your model / tokenizer setup. Nothing else to import.\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "import math\n",
        "import traceback\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from decord import VideoReader, cpu\n",
        "from PIL import Image\n",
        "\n",
        "# ─── Runtime parameters ────────────────────────────────────────────────\n",
        "NUM_FRAMES      = 32          # frames sampled per chunk\n",
        "CLIP_DUR   = 120         # length of each chunk (≈2 min)\n",
        "TEMPERATURE     = 0.1\n",
        "INPUT_SIZE      = 448\n",
        "OUTPUT_JSON     = \"video_descriptions.json\"\n",
        "INPUT_SIZE = 448\n",
        "MEAN = (0.485, 0.456, 0.406)\n",
        "STD = (0.229, 0.224, 0.225)\n",
        "MAX_PATCHES  = 12\n",
        "\n",
        "first_prompt = \"\"\"\n",
        "Be an objective Visual Evidence Analyst. Report on visual data only.\n",
        "\n",
        "Summary:\n",
        "- Overview and chronological events.\n",
        "\n",
        "- Military/Police:\n",
        "  - Appearance: Uniform colors, is there a visable insignia, gear.\n",
        "  - Weapons: Firearm or sticks.\n",
        "  - Force: All instances of physical force.\n",
        "  - Interaction: Commands, arrests, aid etc.\n",
        "\n",
        "- Civilians:\n",
        "  - Actions: Protesting, fleeing, throwing objects, etc.\n",
        "  - Condition: Visible injuries, distress, on ground.\n",
        "\n",
        "- Vehicles:\n",
        "  - Type: Civilian, police, military.\n",
        "  - Markings: Transcribe & translate.\n",
        "\n",
        "- Written Materials:\n",
        "  - Content: Banners, signs, graffiti.\n",
        "  - Translation: Quote & translate.\n",
        "\n",
        "- Conditions:\n",
        "  - Environment: Puddles on the ground indicating rain.\n",
        "\"\"\"\n",
        "\n",
        "second_prompt = \"\"\"\n",
        "Continue acting as an objective Visual Evidence Analyst. Your task is to produce a single, updated cumulative report.\n",
        "\n",
        "\n",
        "\n",
        "**Instructions:**\n",
        "\n",
        "1.  Take the analysis from the previous context as your starting point.\n",
        "2.  Integrate any new visual information from the current clip into the appropriate categories below.\n",
        "3.  Carry forward all previously seen details, even if they are not visible in the current clip. \n",
        "    The final output must be a complete record of everything seen so far.\n",
        "4.  Do not create a separate \"new observations\" section.\n",
        "\n",
        "- Overview and chronological events\n",
        "- Military/Police\n",
        "  - Appearance\n",
        "  - Weapons\n",
        "  - Force\n",
        "  - Interaction\n",
        "- Civilians\n",
        "  - Actions\n",
        "  - Condition\n",
        "- Vehicles\n",
        "  - Type\n",
        "  - Markings\n",
        "- Written Materials\n",
        "  - Content\n",
        "  - Translation\n",
        "- Conditions\n",
        "  - Puddles suggesting it might have rained\n",
        "\"\"\"\n",
        "\n",
        "GEN_CONFIG = dict(max_new_tokens=1024, do_sample=True, temperature=TEMPERATURE)\n",
        "\n",
        "# ─── Build the transform once (uses the helper you supplied) ────────────\n",
        "def transform_video(img):\n",
        "    tfm = T.Compose([\n",
        "        T.Lambda(lambda x: x.convert('RGB')),\n",
        "        T.Resize((INPUT_SIZE, INPUT_SIZE), interpolation=InterpolationMode.BICUBIC),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize(mean=MEAN, std=STD)\n",
        "    ])\n",
        "    return tfm(img)\n",
        "\n",
        "# ─── Frame sampling + decoding ──────────────────────────────────────────\n",
        "def get_index(clip_time_range, fps, max_frame, first_idx=0, num_segments=32):\n",
        "    if clip_time_range:\n",
        "        start, end = clip_time_range[0], clip_time_range[1]\n",
        "    else: # If no time range is specified will take entire video \n",
        "        start, end = -100000, 100000\n",
        "    start_idx = max(first_idx, round(start * fps))\n",
        "    end_idx = min(round(end * fps), max_frame)\n",
        "    seg_size = float(end_idx - start_idx) / num_segments\n",
        "    frame_indices = np.array([\n",
        "        int(start_idx + (seg_size / 2) + np.round(seg_size * idx))\n",
        "        for idx in range(num_segments)\n",
        "    ])\n",
        "    return frame_indices\n",
        "\n",
        "def dynamic_preprocess(image, image_size=448, max_num=12):\n",
        "    orig_w, orig_h = image.size\n",
        "    aspect = orig_w / orig_h\n",
        "\n",
        "    # find best grid (i × j) closest to aspect, with i*j ≤ max_num\n",
        "    best, best_diff = (1,1), float('inf')\n",
        "    for i in range(1, max_num+1):\n",
        "        for j in range(1, max_num+1):\n",
        "            if i*j > max_num: continue\n",
        "            diff = abs(aspect - (i/j))\n",
        "            if diff < best_diff:\n",
        "                best, best_diff = (i,j), diff\n",
        "\n",
        "    gw, gh = best\n",
        "    new_w, new_h = image_size * gw, image_size * gh\n",
        "    image = image.resize((new_w, new_h))\n",
        "\n",
        "    tiles = []\n",
        "    for y in range(gh):\n",
        "        for x in range(gw):\n",
        "            box = (x*image_size, y*image_size, (x+1)*image_size, (y+1)*image_size)\n",
        "            tiles.append(image.crop(box))\n",
        "    return tiles\n",
        "\n",
        "def load_video_frames(video_path, start_s, end_s, num_segments=NUM_FRAMES):\n",
        "    \"\"\"Return a tensor `(T,C,H,W)` and the frame indices used.\"\"\"\n",
        "    # Get video properties \n",
        "    vr   = VideoReader(video_path, ctx=(cpu(0)))\n",
        "    fps  = vr.get_avg_fps()\n",
        "    last = len(vr) - 1\n",
        "\n",
        "    # Get index of frames \n",
        "    idxs = get_index([start_s, end_s], fps, last, num_segments=num_segments)\n",
        "    if len(idxs) == 0:\n",
        "        return torch.empty(0), idxs\n",
        "    \n",
        "    # Create a batch of frames for inference \n",
        "    frames = vr.get_batch(idxs.tolist())       # decord.NDArray\n",
        "    frames = frames.asnumpy()       \n",
        "    tensors = []\n",
        "    for fr_np in frames:\n",
        "        img   = Image.fromarray(np.asarray(fr_np)).convert(\"RGB\")\n",
        "        tiles = dynamic_preprocess(img, \n",
        "                                   image_size=INPUT_SIZE,\n",
        "                                   max_num=1)\n",
        "        tensors.append(transform_video(tiles[0]))\n",
        "    return torch.stack(tensors), idxs\n",
        "\n",
        "\n",
        "def clip_as_prompt(n):\n",
        "    \"\"\"Generate ‘Frame1: <image>…’ placeholders for the prompt.\"\"\"\n",
        "    return \"\\n\".join(f\"Frame{i+1}: <image>\" for i in range(n))\n",
        "\n",
        "\n",
        "# ─── Main loop ─────────────────────────────────────────────────────────\n",
        "def describe_video(video_path, clip_dur=CLIP_DUR):\n",
        "    results, prev_output = [], \"\"\n",
        "    video_path = Path(video_path)\n",
        "\n",
        "    try:\n",
        "        meta_vr  = VideoReader(str(video_path), ctx=cpu(0))\n",
        "        fps      = meta_vr.get_avg_fps()\n",
        "        duration_float = len(meta_vr) / fps\n",
        "    except Exception:\n",
        "        print(\"❌ Failed to open video:\", traceback.format_exc())\n",
        "        return []\n",
        "\n",
        "    print(f\"▶ Processing {video_path.name} ({duration_float:.1f}s)\")\n",
        "\n",
        "    duration_int = int(duration_float + 1e-6) # Convert to int and round up\n",
        "\n",
        "    for start in range(0, duration_int, clip_dur):\n",
        "        end = min(start + clip_dur, duration_int)\n",
        "        print(f\" Segment {start:>5.1f}s – {end:>6.1f}s\")\n",
        "\n",
        "        # Load frames\n",
        "        vid_t, frame_indices = load_video_frames(str(video_path), start, end)\n",
        "        \n",
        "        # Convert to list and skip if empty \n",
        "        if isinstance(frame_indices, np.ndarray):\n",
        "            frame_indices = frame_indices.flatten().tolist()\n",
        "        if vid_t.numel() == 0:\n",
        "            continue\n",
        "        \n",
        "        # Shape tensor and move to GPU\n",
        "        vid_t = vid_t.to(torch.bfloat16).cuda().contiguous()  # (1,T,C,H,W)\n",
        "        \n",
        "        # Build prompt\n",
        "        if start == 0:\n",
        "          prompt = first_prompt\n",
        "        \n",
        "        else:\n",
        "          context = f\"The previous clip showed: {prev_output}\\n\\n\" if prev_output else \"\"\n",
        "          prompt   = (\n",
        "              f\"This is what you found in a previous part of this video:\\n{context}\"\n",
        "              f\"{clip_as_prompt(len(frame_indices))}\\n\"\n",
        "              f\"{second_prompt}\"\n",
        "          )\n",
        "\n",
        "        response, _ = model.chat(\n",
        "            tokenizer, vid_t, prompt, GEN_CONFIG,\n",
        "            history=None, return_history=True\n",
        "        )\n",
        "\n",
        "        prev_output = response\n",
        "\n",
        "        results.append({\"start_sec\": start, \n",
        "                        \"end_sec\": end, \n",
        "                        \"description\": response})\n",
        "        \n",
        "        del vid_t\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    # ── Save JSON ────────────────────────────────────────────────────\n",
        "    with open(OUTPUT_JSON, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(results, f, indent=2)\n",
        "    print(f\"✔ All chunk summaries saved to {OUTPUT_JSON}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# ─── main ───────────────────────────────────────────────────────────\n",
        "if __name__ == \"__main__\":\n",
        "    describe_video(\"toy_ds/TNS_0001_V.mp4\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "videoqa",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
