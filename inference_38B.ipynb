{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c6fecd0e",
      "metadata": {
        "id": "c6fecd0e"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e195b9e",
      "metadata": {
        "id": "7e195b9e"
      },
      "outputs": [],
      "source": [
        "# #Make a directory for pretrained models\n",
        "# !mkdir -p pretrained\n",
        "# %cd pretrained\n",
        "\n",
        "# #Download the model from Hugging Face into the directory\n",
        "# !huggingface-cli download --resume-download \\\n",
        "#   --local-dir-use-symlinks False \\\n",
        "#   --local-dir InternVL3-38B \\\n",
        "#   OpenGVLab/InternVL3-38B"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Video reader\n",
        "%pip install decord\n",
        "\n",
        "%pip install -U bitsandbytes\n",
        "\n",
        "# FlashAttention (for use_flash_attn=True)\n",
        "%pip install \"https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.12/flash_attn-2.8.0+cu124torch2.4-cp311-cp311-linux_x86_64.whl\""
      ],
      "metadata": {
        "id": "68S23q2iZUy4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66e9625d-bb30-475a-a984-24a71b9b37c9"
      },
      "id": "68S23q2iZUy4",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: decord in /usr/local/lib/python3.11/dist-packages (0.6.0)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from decord) (2.0.2)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.46.0)\n",
            "Requirement already satisfied: torch<3,>=2.2 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.2->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (3.0.2)\n",
            "Collecting flash-attn==2.8.0+cu124torch2.4\n",
            "  Downloading https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.12/flash_attn-2.8.0+cu124torch2.4-cp311-cp311-linux_x86_64.whl (115.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.2/115.2 MB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from flash-attn==2.8.0+cu124torch2.4) (2.6.0+cu124)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from flash-attn==2.8.0+cu124torch2.4) (0.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.8.0+cu124torch2.4) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.8.0+cu124torch2.4) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.8.0+cu124torch2.4) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.8.0+cu124torch2.4) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.8.0+cu124torch2.4) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.8.0+cu124torch2.4) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.8.0+cu124torch2.4) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.8.0+cu124torch2.4) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.8.0+cu124torch2.4) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.8.0+cu124torch2.4) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.8.0+cu124torch2.4) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.8.0+cu124torch2.4) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.8.0+cu124torch2.4) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.8.0+cu124torch2.4) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.8.0+cu124torch2.4) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.8.0+cu124torch2.4) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.8.0+cu124torch2.4) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.8.0+cu124torch2.4) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.8.0+cu124torch2.4) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.8.0+cu124torch2.4) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->flash-attn==2.8.0+cu124torch2.4) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->flash-attn==2.8.0+cu124torch2.4) (3.0.2)\n",
            "Installing collected packages: flash-attn\n",
            "Successfully installed flash-attn-2.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "Zvm8XSQGZOb2",
      "metadata": {
        "id": "Zvm8XSQGZOb2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06d4e621-575d-4294-8412-16da08cc7b44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "toy_ds = '/content/drive/My Drive/InternVL/toy_ds'\n",
        "MODEL_PATH = '/content/drive/MyDrive/InternVL/InternVL/InternVL3-38B'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "241c4d49",
      "metadata": {
        "id": "241c4d49"
      },
      "source": [
        "## Video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "283f33fc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "id": "283f33fc",
        "outputId": "a30e15d8-0a43-453b-906d-41a1c878fa05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2-637971590.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mTEMP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# ─── Model Setup 78B 8 bit quant ─────────────────────────────────────────────────\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m model = AutoModel.from_pretrained(\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0mMODEL_PATH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbfloat16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mmodel_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_generation_mixin_to_remote_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 564\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    565\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0mold_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4553\u001b[0m         \u001b[0;31m# Prepare the full device map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4554\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdevice_map\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4555\u001b[0;31m             \u001b[0mdevice_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_device_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_memory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhf_quantizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_in_fp32_regex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4557\u001b[0m         \u001b[0;31m# Finalize model weight initialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_get_device_map\u001b[0;34m(model, device_map, max_memory, hf_quantizer, torch_dtype, keep_in_fp32_regex)\u001b[0m\n\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhf_quantizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1344\u001b[0;31m             \u001b[0mhf_quantizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_environment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdevice_map\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/quantizers/quantizer_bnb_8bit.py\u001b[0m in \u001b[0;36mvalidate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     99\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdevice_map_without_lm_head\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"disk\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdevice_map_without_lm_head\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    102\u001b[0m                     \u001b[0;34m\"Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                     \u001b[0;34m\"quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. "
          ]
        }
      ],
      "source": [
        "import os\n",
        "import math\n",
        "import torch\n",
        "import numpy as np\n",
        "import torchvision.transforms as T\n",
        "from decord import VideoReader, cpu\n",
        "from PIL import Image\n",
        "from torchvision.transforms.functional import InterpolationMode\n",
        "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
        "\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "# ─── Prompt ─────────────────────────────────────────────────\n",
        "prompt_string = \"\"\"\n",
        "Please write one detailed but concise paragraph describing the following, based on the video frames or images above:\n",
        "- The overall scene and weather conditions (e.g., rain, puddles).\n",
        "- Any visible weapons or use of force:\n",
        "  • Guns (how many and where)\n",
        "  • Non-lethal force (e.g., hitting with fists, stones, or sticks)\n",
        "- Vehicles in the scene:\n",
        "  • Presence of military vehicles (specify type)\n",
        "  • License plates or markings (quote exact text)\n",
        "- Human activity:\n",
        "  • Actions of uniformed personnel (include uniform colors)\n",
        "  • Actions of protestors\n",
        "  • Whether anyone appears to be injured, hurt, or lying on the ground\n",
        "\"\"\"\n",
        "\n",
        "TEMP = 0.2\n",
        "# ─── Model Setup 78B 8 bit quant ─────────────────────────────────────────────────\n",
        "model = AutoModel.from_pretrained(\n",
        "    MODEL_PATH,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    load_in_8bit=True,\n",
        "    low_cpu_mem_usage=True,\n",
        "    use_flash_attn=True,\n",
        "    trust_remote_code=True).eval()\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)\n",
        "\n",
        "# ─── Constants ───────────────────────────────────────────────────\n",
        "INPUT_SIZE = 448\n",
        "NUM_FRAMES = 28\n",
        "GEN_CONFIG = dict(max_new_tokens=1024,\n",
        "                  do_sample=True,\n",
        "                  temperature=TEMP)\n",
        "\n",
        "MEAN = (0.485, 0.456, 0.406)\n",
        "STD = (0.229, 0.224, 0.225)\n",
        "\n",
        "# ─── Transforms ─────────────────────────────────────────────────\n",
        "def build_transform():\n",
        "    return T.Compose([\n",
        "        T.Lambda(lambda img: img.convert('RGB')),\n",
        "        T.Resize((INPUT_SIZE, INPUT_SIZE), interpolation=InterpolationMode.BICUBIC),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize(mean=MEAN, std=STD)\n",
        "    ])\n",
        "\n",
        "# ─── Load and Preprocess Video ──────────────────────────────────\n",
        "def get_frame_indices(num_frames, total):\n",
        "    return np.linspace(0, total - 1, num_frames, dtype=int)\n",
        "\n",
        "def load_video(video_path, num_frames):\n",
        "    vr = VideoReader(video_path, ctx=cpu(0))\n",
        "    transform = build_transform()\n",
        "    indices = get_frame_indices(num_frames, len(vr))\n",
        "    pixel_values = [transform(Image.fromarray(vr[i].asnumpy())) for i in indices]\n",
        "    return torch.stack(pixel_values)  # [num_frames, 3, H, W]\n",
        "\n",
        "# ─── Inference ──────────────────────────────────────────────────\n",
        "def infer(video_path):\n",
        "    video_tensor = load_video(video_path, NUM_FRAMES).to(torch.bfloat16).cuda()\n",
        "    prompt = ''.join([f'Frame{i+1}: <image>\\n' for i in range(NUM_FRAMES)])\n",
        "    prompt += prompt_string\n",
        "\n",
        "    response, _ = model.chat(\n",
        "        tokenizer,\n",
        "        video_tensor,\n",
        "        prompt,\n",
        "        GEN_CONFIG,\n",
        "        history=None,\n",
        "        return_history=True\n",
        "    )\n",
        "    print(\"User:\", prompt)\n",
        "    print(\"Assistant:\", response)\n",
        "\n",
        "# ─── Entry ──────────────────────────────────────────────────────\n",
        "if __name__ == \"__main__\":\n",
        "    infer(\"/content/drive/MyDrive/InternVL/toy_ds/videos/TNS_0169_V.mp4\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a1aae66",
      "metadata": {
        "id": "9a1aae66"
      },
      "source": [
        "## Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "620df763",
      "metadata": {
        "id": "620df763"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "import torchvision.transforms as T\n",
        "from PIL import Image\n",
        "from torchvision.transforms.functional import InterpolationMode\n",
        "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
        "\n",
        "# ─── Prompt ─────────────────────────────────────────────────\n",
        "prompt_string = \"Provide a detailed description of each image. Describe the foreground and background separately. Mention any people, objects, and actions clearly. What are the people doing? What expressions or activities are visible? What is the setting or context? Is there violence happening?\"\n",
        "\n",
        "# ─── Config ───────────────────────────────────────────────────────────────\n",
        "MODEL_PATH = \"./pretrained/InternVL3-78B\"\n",
        "INPUT_SIZE   = 448\n",
        "MAX_PATCHES  = 12\n",
        "GEN_CONFIG   = dict(max_new_tokens=1024, do_sample=True)\n",
        "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
        "IMAGENET_STD  = (0.229, 0.224, 0.225)\n",
        "\n",
        "# ─── Split Model Across GPUs ──────────────────────────────────────────────\n",
        "def split_model(model_name):\n",
        "    device_map = {}\n",
        "    world_size = torch.cuda.device_count()\n",
        "    config = AutoConfig.from_pretrained(model_name, trust_remote_code=True)\n",
        "    num_layers = config.llm_config.num_hidden_layers\n",
        "\n",
        "    # Distribute layers evenly, but give half of GPU0 to the vision part\n",
        "    num_layers_per_gpu = math.ceil(num_layers / (world_size - 0.5))\n",
        "    num_layers_per_gpu = [num_layers_per_gpu] * world_size\n",
        "    num_layers_per_gpu[0] = math.ceil(num_layers_per_gpu[0] * 0.5)\n",
        "\n",
        "    layer_cnt = 0\n",
        "    for gpu_idx, count in enumerate(num_layers_per_gpu):\n",
        "        for _ in range(count):\n",
        "            device_map[f'language_model.model.layers.{layer_cnt}'] = gpu_idx\n",
        "            layer_cnt += 1\n",
        "\n",
        "    # Pin all vision & shared embeddings to GPU0\n",
        "    vision_keys = [\n",
        "        'vision_model',\n",
        "        'mlp1',\n",
        "        'language_model.model.tok_embeddings',\n",
        "        'language_model.model.embed_tokens',\n",
        "        'language_model.output',\n",
        "        'language_model.model.norm',\n",
        "        'language_model.model.rotary_emb',\n",
        "        'language_model.lm_head',\n",
        "        f'language_model.model.layers.{num_layers - 1}'\n",
        "    ]\n",
        "    for key in vision_keys:\n",
        "        device_map[key] = 0\n",
        "\n",
        "    return device_map\n",
        "\n",
        "# ─── Image Preprocessing ─────────────────────────────────────────────────\n",
        "def build_transform():\n",
        "    return T.Compose([\n",
        "        T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
        "        T.Resize((INPUT_SIZE, INPUT_SIZE), interpolation=InterpolationMode.BICUBIC),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
        "    ])\n",
        "\n",
        "def dynamic_preprocess(image, image_size=448, max_num=12):\n",
        "    orig_w, orig_h = image.size\n",
        "    aspect = orig_w / orig_h\n",
        "\n",
        "    # find best grid (i × j) closest to aspect, with i*j ≤ max_num\n",
        "    best, best_diff = (1,1), float('inf')\n",
        "    for i in range(1, max_num+1):\n",
        "        for j in range(1, max_num+1):\n",
        "            if i*j > max_num: continue\n",
        "            diff = abs(aspect - (i/j))\n",
        "            if diff < best_diff:\n",
        "                best, best_diff = (i,j), diff\n",
        "\n",
        "    gw, gh = best\n",
        "    new_w, new_h = image_size * gw, image_size * gh\n",
        "    image = image.resize((new_w, new_h))\n",
        "\n",
        "    tiles = []\n",
        "    for y in range(gh):\n",
        "        for x in range(gw):\n",
        "            box = (x*image_size, y*image_size, (x+1)*image_size, (y+1)*image_size)\n",
        "            tiles.append(image.crop(box))\n",
        "    return tiles\n",
        "\n",
        "def load_image(path, image_size=448, max_num=12):\n",
        "    img = Image.open(path).convert('RGB')\n",
        "    tiles = dynamic_preprocess(img, image_size=image_size, max_num=max_num)\n",
        "    tfm = build_transform()\n",
        "    return torch.stack([tfm(t) for t in tiles])\n",
        "\n",
        "# ─── Inference ────────────────────────────────────────────────────────────\n",
        "def infer(image_path):\n",
        "    device_map = split_model(MODEL_PATH)\n",
        "    model     = AutoModel.from_pretrained(\n",
        "                    MODEL_PATH,\n",
        "                    torch_dtype=torch.bfloat16,\n",
        "                    low_cpu_mem_usage=True,\n",
        "                    use_flash_attn=True,\n",
        "                    trust_remote_code=True,\n",
        "                    device_map=device_map\n",
        "                ).eval()\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True, use_fast=False)\n",
        "\n",
        "    pixel_values = load_image(image_path, max_num=MAX_PATCHES)\n",
        "    pixel_values = pixel_values.to(torch.bfloat16).cuda()\n",
        "\n",
        "    question = f\"<image>\\n{prompt_string}\"\n",
        "    response, _ = model.chat(tokenizer, pixel_values, question, GEN_CONFIG, history=None, return_history=True)\n",
        "\n",
        "    print(\"User:\", question)\n",
        "    print(\"Assistant:\", response)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    infer(\"toy_ds/images/TNS_3773_I.jpg\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4aab4ef2"
      },
      "source": [],
      "id": "4aab4ef2",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}