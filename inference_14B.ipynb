{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c6fecd0e",
      "metadata": {
        "id": "c6fecd0e"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "82cf2f21",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/jupyter-dai7591/.conda/envs/videoqa/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00, 14.96it/s]\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import torchvision.transforms as T\n",
        "from decord import VideoReader, cpu\n",
        "from PIL import Image\n",
        "from torchvision.transforms.functional import InterpolationMode\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "import json\n",
        "\n",
        "\n",
        "# ─── Model Setup 38B 8 bit quant ─────────────────────────────────────────────────\n",
        "MODEL_PATH = 'pretrained/InternVL3-14B'\n",
        "model = AutoModel.from_pretrained(\n",
        "    MODEL_PATH,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    low_cpu_mem_usage=True,\n",
        "    use_flash_attn=True,\n",
        "    trust_remote_code=True).eval().cuda()\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "241c4d49",
      "metadata": {
        "id": "241c4d49"
      },
      "source": [
        "## Video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "23ffe2e0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ─── Variables ───────────────────────────────────────────────────\n",
        "num_frames = 32\n",
        "temp = 0.1\n",
        "# ─── Prompt ─────────────────────────────────────────────────\n",
        "prompt_string = \"\"\"\n",
        "Analyze visual for a human rights report. Be an objective Visual Evidence Analyst. \n",
        "Report on visual/audible data only. Use this template, focusing on violations \n",
        "(force, weapons, injuries).\n",
        "\n",
        "Summary:\n",
        "- Overview and chronological events.\n",
        "\n",
        "2. Actors:\n",
        "- Military/Police:\n",
        "  - Appearance: Uniforms, colors, insignia, gear.\n",
        "  - Equipment: Shields, batons, etc.\n",
        "  - Weapons: Firearm types and use.\n",
        "  - Force: All instances of physical force.\n",
        "  - Interaction: Commands, arrests, aid.\n",
        "- Civilians:\n",
        "  - Actions: Protesting, fleeing, throwing objects, etc.\n",
        "  - Condition: Visible injuries, distress, on ground.\n",
        "\n",
        "3. Environment & Objects:**\n",
        "- Vehicles:\n",
        "  - Type: Civilian, police, military.\n",
        "  - Markings: Transcribe & translate.\n",
        "- Written Materials:\n",
        "  - Content: Banners, signs, graffiti.\n",
        "  - Translation: Quote & translate.\n",
        "- Conditions:\n",
        "  - Environment: Puddles on the ground indicating rain. \n",
        "\"\"\"\n",
        "\n",
        "# ─── Constants ───────────────────────────────────────────────────\n",
        "INPUT_SIZE = 448\n",
        "GEN_CONFIG = dict(max_new_tokens=1024, do_sample=True, temperature=temp)\n",
        "MEAN = (0.485, 0.456, 0.406)\n",
        "STD = (0.229, 0.224, 0.225)\n",
        "MAX_PATCHES  = 12\n",
        "OUTPUT_JSON = \"video_descriptions.json\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96fc60fb",
      "metadata": {},
      "source": [
        "## Helpers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "283f33fc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 597
        },
        "id": "283f33fc",
        "outputId": "5bc56957-fda5-4d86-d582-1168e2461c4f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
          ]
        }
      ],
      "source": [
        "# ─── Video  Process ──────────────────────────────────\n",
        "def transform_video():\n",
        "    return T.Compose([\n",
        "        T.Lambda(lambda img: img.convert('RGB')),\n",
        "        T.Resize((INPUT_SIZE, INPUT_SIZE), interpolation=InterpolationMode.BICUBIC),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize(mean=MEAN, std=STD)\n",
        "    ])\n",
        "\n",
        "def get_frame_indices(num_frames, total):\n",
        "    return np.linspace(0, total - 1, num_frames, dtype=int)\n",
        "\n",
        "def load_video(video_path, num_frames):\n",
        "    vr = VideoReader(video_path, ctx=cpu(0))\n",
        "    transform = transform_video()\n",
        "    indices = get_frame_indices(num_frames, len(vr))\n",
        "    pixel_values = [transform(Image.fromarray(vr[i].asnumpy())) for i in indices]\n",
        "    return torch.stack(pixel_values)  # [num_frames, 3, H, W]\n",
        "\n",
        "def infer_video(video_path):\n",
        "    try:\n",
        "        video_tensor = load_video(video_path, num_frames).to(torch.bfloat16).cuda()\n",
        "        video_tensor = video_tensor.contiguous()\n",
        "\n",
        "        prompt = ''.join([f'Frame{i+1}: <image>\\n' for i in range(num_frames)])\n",
        "        prompt += prompt_string\n",
        "\n",
        "        response, _ = model.chat(\n",
        "            tokenizer,\n",
        "            video_tensor,\n",
        "            prompt,\n",
        "            GEN_CONFIG,\n",
        "            history=None,\n",
        "            return_history=True\n",
        "        )\n",
        "        return response\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\"\n",
        "\n",
        "\n",
        "\n",
        "# ─── Image Preprocessing ─────────────────────────────────────────────────\n",
        "def transform_img():\n",
        "    return T.Compose([\n",
        "        T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
        "        T.Resize((INPUT_SIZE, INPUT_SIZE), interpolation=InterpolationMode.BICUBIC),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize(mean=MEAN, std=STD),\n",
        "    ])\n",
        "\n",
        "def dynamic_preprocess(image, image_size=448, max_num=12):\n",
        "    orig_w, orig_h = image.size\n",
        "    aspect = orig_w / orig_h\n",
        "\n",
        "    # find best grid (i × j) closest to aspect, with i*j ≤ max_num\n",
        "    best, best_diff = (1,1), float('inf')\n",
        "    for i in range(1, max_num+1):\n",
        "        for j in range(1, max_num+1):\n",
        "            if i*j > max_num: continue\n",
        "            diff = abs(aspect - (i/j))\n",
        "            if diff < best_diff:\n",
        "                best, best_diff = (i,j), diff\n",
        "\n",
        "    gw, gh = best\n",
        "    new_w, new_h = image_size * gw, image_size * gh\n",
        "    image = image.resize((new_w, new_h))\n",
        "\n",
        "    tiles = []\n",
        "    for y in range(gh):\n",
        "        for x in range(gw):\n",
        "            box = (x*image_size, y*image_size, (x+1)*image_size, (y+1)*image_size)\n",
        "            tiles.append(image.crop(box))\n",
        "    return tiles\n",
        "\n",
        "def load_image(path, image_size=448, max_num=12):\n",
        "    img = Image.open(path).convert('RGB')\n",
        "    tiles = dynamic_preprocess(img, image_size=image_size, max_num=max_num)\n",
        "    tfm = transform_img()\n",
        "    return torch.stack([tfm(t) for t in tiles])\n",
        "\n",
        "# ─── Inference ────────────────────────────────────────────────────────────\n",
        "def infer_img(image_path):\n",
        "    try:\n",
        "        pixel_values = load_image(image_path, max_num=MAX_PATCHES).to(torch.bfloat16).cuda()\n",
        "        pixel_values = pixel_values.contiguous()\n",
        "\n",
        "\n",
        "        question = f\"<image>\\n{prompt_string}\"\n",
        "        response, _ = model.chat(tokenizer, \n",
        "                                pixel_values, \n",
        "                                question, \n",
        "                                GEN_CONFIG, \n",
        "                                history=None, \n",
        "                                return_history=True)\n",
        "\n",
        "        return response\n",
        "    \n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\"\n",
        "\n",
        "# ─── Batch Loop ──────────────────────────────────────────────\n",
        "def batch_infer(video_dir):\n",
        "    results = {}\n",
        "    for filename in sorted(os.listdir(video_dir)):\n",
        "        if filename.lower().endswith(\".mp4\"):\n",
        "            path = os.path.join(video_dir, filename)\n",
        "            description = infer_video(path)\n",
        "            results[filename] = description\n",
        "\n",
        "        elif filename.lower().endswith('.jpg'):\n",
        "            path = os.path.join(video_dir, filename)\n",
        "            description = infer_img(path)\n",
        "            results[filename] = description\n",
        "    return results\n",
        "\n",
        "# ─── Main ───────────────────────────────────────────────────\n",
        "if __name__ == \"__main__\":\n",
        "    results = batch_infer('toy_ds')\n",
        "    with open(OUTPUT_JSON, \"w\") as f:\n",
        "        json.dump(results, f, indent=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a1aae66",
      "metadata": {
        "id": "9a1aae66"
      },
      "source": [
        "## Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "620df763",
      "metadata": {
        "id": "620df763"
      },
      "outputs": [],
      "source": [
        "# import math\n",
        "# import torch\n",
        "# import torchvision.transforms as T\n",
        "# from PIL import Image\n",
        "# from torchvision.transforms.functional import InterpolationMode\n",
        "# from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
        "\n",
        "# # ─── Prompt ─────────────────────────────────────────────────\n",
        "# prompt_string = \"Provide a detailed description of each image. Describe the foreground and background separately. Mention any people, objects, and actions clearly. What are the people doing? What expressions or activities are visible? What is the setting or context? Is there violence happening?\"\n",
        "\n",
        "# # ─── Config ───────────────────────────────────────────────────────────────\n",
        "# MODEL_PATH = \"./pretrained/InternVL3-78B\"\n",
        "# INPUT_SIZE   = 448\n",
        "# MAX_PATCHES  = 12\n",
        "# GEN_CONFIG   = dict(max_new_tokens=1024, do_sample=True)\n",
        "# MEAN = (0.485, 0.456, 0.406)\n",
        "# IMAGENET_STD  = (0.229, 0.224, 0.225)\n",
        "\n",
        "# # ─── Split Model Across GPUs ──────────────────────────────────────────────\n",
        "# def split_model(model_name):\n",
        "#     device_map = {}\n",
        "#     world_size = torch.cuda.device_count()\n",
        "#     config = AutoConfig.from_pretrained(model_name, trust_remote_code=True)\n",
        "#     num_layers = config.llm_config.num_hidden_layers\n",
        "\n",
        "#     # Distribute layers evenly, but give half of GPU0 to the vision part\n",
        "#     num_layers_per_gpu = math.ceil(num_layers / (world_size - 0.5))\n",
        "#     num_layers_per_gpu = [num_layers_per_gpu] * world_size\n",
        "#     num_layers_per_gpu[0] = math.ceil(num_layers_per_gpu[0] * 0.5)\n",
        "\n",
        "#     layer_cnt = 0\n",
        "#     for gpu_idx, count in enumerate(num_layers_per_gpu):\n",
        "#         for _ in range(count):\n",
        "#             device_map[f'language_model.model.layers.{layer_cnt}'] = gpu_idx\n",
        "#             layer_cnt += 1\n",
        "\n",
        "#     # Pin all vision & shared embeddings to GPU0\n",
        "#     vision_keys = [\n",
        "#         'vision_model',\n",
        "#         'mlp1',\n",
        "#         'language_model.model.tok_embeddings',\n",
        "#         'language_model.model.embed_tokens',\n",
        "#         'language_model.output',\n",
        "#         'language_model.model.norm',\n",
        "#         'language_model.model.rotary_emb',\n",
        "#         'language_model.lm_head',\n",
        "#         f'language_model.model.layers.{num_layers - 1}'\n",
        "#     ]\n",
        "#     for key in vision_keys:\n",
        "#         device_map[key] = 0\n",
        "\n",
        "#     return device_map\n",
        "\n",
        "# # ─── Image Preprocessing ─────────────────────────────────────────────────\n",
        "# def transform_video():\n",
        "#     return T.Compose([\n",
        "#         T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
        "#         T.Resize((INPUT_SIZE, INPUT_SIZE), interpolation=InterpolationMode.BICUBIC),\n",
        "#         T.ToTensor(),\n",
        "#         T.Normalize(mean=MEAN, std=IMAGENET_STD),\n",
        "#     ])\n",
        "\n",
        "# def dynamic_preprocess(image, image_size=448, max_num=12):\n",
        "#     orig_w, orig_h = image.size\n",
        "#     aspect = orig_w / orig_h\n",
        "\n",
        "#     # find best grid (i × j) closest to aspect, with i*j ≤ max_num\n",
        "#     best, best_diff = (1,1), float('inf')\n",
        "#     for i in range(1, max_num+1):\n",
        "#         for j in range(1, max_num+1):\n",
        "#             if i*j > max_num: continue\n",
        "#             diff = abs(aspect - (i/j))\n",
        "#             if diff < best_diff:\n",
        "#                 best, best_diff = (i,j), diff\n",
        "\n",
        "#     gw, gh = best\n",
        "#     new_w, new_h = image_size * gw, image_size * gh\n",
        "#     image = image.resize((new_w, new_h))\n",
        "\n",
        "#     tiles = []\n",
        "#     for y in range(gh):\n",
        "#         for x in range(gw):\n",
        "#             box = (x*image_size, y*image_size, (x+1)*image_size, (y+1)*image_size)\n",
        "#             tiles.append(image.crop(box))\n",
        "#     return tiles\n",
        "\n",
        "# def load_image(path, image_size=448, max_num=12):\n",
        "#     img = Image.open(path).convert('RGB')\n",
        "#     tiles = dynamic_preprocess(img, image_size=image_size, max_num=max_num)\n",
        "#     tfm = transform_video()\n",
        "#     return torch.stack([tfm(t) for t in tiles])\n",
        "\n",
        "# # ─── Inference ────────────────────────────────────────────────────────────\n",
        "# def infer(image_path):\n",
        "#     device_map = split_model(MODEL_PATH)\n",
        "#     model     = AutoModel.from_pretrained(\n",
        "#                     MODEL_PATH,\n",
        "#                     torch_dtype=torch.bfloat16,\n",
        "#                     low_cpu_mem_usage=True,\n",
        "#                     use_flash_attn=True,\n",
        "#                     trust_remote_code=True,\n",
        "#                     device_map=device_map\n",
        "#                 ).eval()\n",
        "#     tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True, use_fast=False)\n",
        "\n",
        "#     pixel_values = load_image(image_path, max_num=MAX_PATCHES)\n",
        "#     pixel_values = pixel_values.to(torch.bfloat16).cuda()\n",
        "\n",
        "#     question = f\"<image>\\n{prompt_string}\"\n",
        "#     response, _ = model.chat(tokenizer, pixel_values, question, GEN_CONFIG, history=None, return_history=True)\n",
        "\n",
        "#     print(\"User:\", question)\n",
        "#     print(\"Assistant:\", response)\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     infer(\"toy_ds/images/TNS_3773_I.jpg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "839e1990",
      "metadata": {},
      "outputs": [],
      "source": [
        "# import os\n",
        "# import torch\n",
        "# import json\n",
        "# import numpy as np\n",
        "# import torchvision.transforms as T\n",
        "# from decord import VideoReader, cpu\n",
        "# from PIL import Image\n",
        "# from torchvision.transforms.functional import InterpolationMode\n",
        "# from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "# # ─── Prompt ─────────────────────────────────────────────────\n",
        "# prompt_string = \"\"\"\n",
        "# Please write one detailed but concise paragraph describing the following, based on the video frames or images above:\n",
        "# - The overall scene and weather conditions (e.g., rain, puddles).\n",
        "# - Any visible weapons or use of force:\n",
        "#   • Guns (how many and where)\n",
        "#   • Non-lethal force (e.g., hitting with fists, stones, or sticks)\n",
        "# - Vehicles in the scene:\n",
        "#   • Presence of military vehicles (specify type)\n",
        "#   • License plates or markings (quote exact text)\n",
        "# - Human activity:\n",
        "#   • Actions of uniformed personnel (include uniform colors)\n",
        "#   • Actions of protestors\n",
        "#   • Whether anyone appears to be injured, hurt, or lying on the ground\n",
        "# \"\"\"\n",
        "\n",
        "# # ─── Config ─────────────────────────────────────────────────\n",
        "# temp = 0.2\n",
        "# INPUT_SIZE = 448\n",
        "# num_frames = 28\n",
        "# GEN_CONFIG = dict(max_new_tokens=1024, do_sample=True, temperature=temp)\n",
        "# MEAN = (0.485, 0.456, 0.406)\n",
        "# STD = (0.229, 0.224, 0.225)\n",
        "# MODEL_PATH = \"OpenGVLab/InternVL3-78B\"  # Update if needed\n",
        "# VIDEO_DIR = \"/content/drive/MyDrive/InternVL/toy_ds/videos\"  # Update as needed\n",
        "# OUTPUT_JSON = \"video_descriptions.json\"\n",
        "\n",
        "# # ─── Load Model ──────────────────────────────────────────────\n",
        "# model = AutoModel.from_pretrained(\n",
        "#     MODEL_PATH,\n",
        "#     torch_dtype=torch.bfloat16,\n",
        "#     load_in_8bit=True,\n",
        "#     low_cpu_mem_usage=True,\n",
        "#     use_flash_attn=True,\n",
        "#     trust_remote_code=True\n",
        "# ).eval().cuda()\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)\n",
        "\n",
        "# # ─── Transforms ──────────────────────────────────────────────\n",
        "# def build_transform():\n",
        "#     return T.Compose([\n",
        "#         T.Lambda(lambda img: img.convert('RGB')),\n",
        "#         T.Resize((INPUT_SIZE, INPUT_SIZE), interpolation=InterpolationMode.BICUBIC),\n",
        "#         T.ToTensor(),\n",
        "#         T.Normalize(mean=MEAN, std=STD)\n",
        "#     ])\n",
        "\n",
        "# # ─── Frame Selection ─────────────────────────────────────────\n",
        "# def get_frame_indices(num_frames, total):\n",
        "#     return np.linspace(0, total - 1, num_frames, dtype=int)\n",
        "\n",
        "# def load_video(video_path, num_frames):\n",
        "#     vr = VideoReader(video_path, ctx=cpu(0))\n",
        "#     transform = build_transform()\n",
        "#     indices = get_frame_indices(num_frames, len(vr))\n",
        "#     pixel_values = [transform(Image.fromarray(vr[i].asnumpy())) for i in indices]\n",
        "#     return torch.stack(pixel_values)  # [num_frames, 3, H, W]\n",
        "\n",
        "# # ─── Inference ───────────────────────────────────────────────\n",
        "# def infer_video(video_path):\n",
        "#     try:\n",
        "#         video_tensor = load_video(video_path, num_frames).to(torch.bfloat16).cuda()\n",
        "#         prompt = ''.join([f'Frame{i+1}: <image>\\n' for i in range(num_frames)])\n",
        "#         prompt += prompt_string\n",
        "\n",
        "#         response, _ = model.chat(\n",
        "#             tokenizer,\n",
        "#             video_tensor,\n",
        "#             prompt,\n",
        "#             GEN_CONFIG,\n",
        "#             history=None,\n",
        "#             return_history=True\n",
        "#         )\n",
        "#         return response\n",
        "#     except Exception as e:\n",
        "#         return f\"Error: {str(e)}\"\n",
        "\n",
        "# # ─── Batch Loop ──────────────────────────────────────────────\n",
        "# def batch_infer(video_dir):\n",
        "#     results = {}\n",
        "#     for filename in sorted(os.listdir(video_dir)):\n",
        "#         if filename.lower().endswith(\".mp4\"):\n",
        "#             path = os.path.join(video_dir, filename)\n",
        "#             description = infer_video(path)\n",
        "#             results[filename] = description\n",
        "#     return results\n",
        "\n",
        "# # ─── Entry ───────────────────────────────────────────────────\n",
        "# if __name__ == \"__main__\":\n",
        "#     results = batch_infer(VIDEO_DIR)\n",
        "#     with open(OUTPUT_JSON, \"w\") as f:\n",
        "#         json.dump(results, f, indent=2)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "videoqa",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
